---
title: "Proyecto rendimiento de estudiantes"
author: "Leandro Soto Miranda"
date: "2024-10-07"
format: 
  html: 
    toc: true 
    code-fold: true
---

## 1. Definir problema

En este proyecto, analizaremos un conjunto de datos de rendiemiento de estudiantes

## 2. Recopilación de datos

```{python}
# Importación de librerías
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
import warnings
warnings.filterwarnings('ignore', category=FutureWarning)


# Carga de datos
df = pd.read_csv('data.csv', sep=",")

# Resumen estadístico
# Mostrar las primeras filas del dataset
print("Primeras filas del dataset:")
print(df.head())

# Mostrar información general del dataset
print("\nInformación del dataset:")
print(df.info())

# Descripción estadística básica del dataset
print("\nDescripción estadística:")
print(df.describe())

info = df.shape
print("\nLa cantidad de filas y columnas en nuestro dataframe es de:",info)

tipos = df.dtypes
print("\nTipos de datos presentes en el dataset:\n",tipos)
```

## 3. **Análisis de datos por Variable**
## Análisis de datos cuantitativos

```{python}
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Función para el análisis univariado de variables cuantitativas
def analizar_variable_cuantitativa(df, columna):
    mean = np.mean(df[columna])
    median = np.median(df[columna])
    std = np.std(df[columna])
    min_value = np.min(df[columna])
    max_value = np.max(df[columna])

    print(f"Análisis de la Variable '{columna}'")
    print(f"Media: {mean:.2f}")
    print(f"Mediana: {median:.2f}")
    print(f"Desviación Estándar: {std:.2f}")
    print(f"Valor Mínimo: {min_value:.2f}")
    print(f"Valor Máximo: {max_value:.2f}")
    print("\n")

    # Visualización de la distribución (Histograma con KDE)
    plt.figure(figsize=(10, 6))
    sns.histplot(df[columna], bins=30, kde=True, color='#4C72B0')
    plt.title(f'Distribución de {columna}')
    plt.xlabel(columna)
    plt.ylabel('Frecuencia')
    plt.show()

# Aplicar la función a todas las columnas numéricas del dataframe
columnas_cuantitativas = df.select_dtypes(include=['int64', 'float64']).columns
for columna in columnas_cuantitativas:
    analizar_variable_cuantitativa(df, columna)
```

## Análisis de datos cualitativos

```{python}

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Función para el análisis univariado de variables categóricas
def analizar_variable_categorica(df, columna):
    values_counts = df[columna].value_counts()
    moda = values_counts.idxmax()

    print(f"Análisis Univariado de la Variable '{columna}'")
    print(f"Frecuencia de las categorías:\n{values_counts}")
    print(f"Moda (Categoría más frecuente): {moda}")
    print("\n")

    # Visualización de la distribución
    plt.figure(figsize=(10, 6))
    sns.countplot(
        x=columna, 
        data=df[df[columna].isin(values_counts.index)], 
        order=values_counts.index, 
        palette='Set2'
    )
    plt.title(f'Distribución de {columna}')
    plt.xlabel(columna)
    plt.ylabel('Frecuencia')
    plt.xticks(rotation=45)
    plt.show()

# Aplicar la función a todas las columnas categóricas del dataframe
columnas_categoricas = df.select_dtypes(include=['object']).columns
for columna in columnas_categoricas:
    analizar_variable_categorica(df, columna)

```

## Verificar si hay valores nulos en el dataset
### Correccion de valores nulos presentes en el dataser
```{python}
print("\nValores nulos por columna:")
print(df.isnull().sum())
```
```{python}
# Lista de las columnas categóricas que deseas reemplazar por la moda
categorical_columns = ['Parental_Education_Level', 'Distance_from_Home', 'Teacher_Quality']

# Iterar sobre cada columna y reemplazar los valores nulos con la moda
for column in categorical_columns:
    mode_value = df[column].mode()[0]  # Obtener la moda (el valor más frecuente)
    df[column].fillna(mode_value, inplace=True)
    print(f"Valores nulos en '{column}' reemplazados por la moda: '{mode_value}'")

```

```{python}
print("\nValores nulos por columna:")
print(df.isnull().sum())
```

# Verificar si hay valores atopicos
```{python}
def generar_boxplot(df, columna):
    plt.figure(figsize=(10, 6))
    sns.boxplot(data=df, x=columna, palette='Set2')
    plt.title(f'Boxplot de {columna}')
    plt.xlabel(columna)
    plt.show()

# Aplicar la función a todas las columnas numéricas del dataframe
columnas_numericas = df.select_dtypes(include=['int64', 'float64']).columns
for columna in columnas_numericas:
    generar_boxplot(df, columna)
```

## Preparacion de datos
### Vamos a realizar el tratamiento de datos pasos a realizar
### Convertir datos categoricos a numericos para esto vamos a aplicar one hot encoder y label conder dependiendo a tipo de dato que se trate
### Ademas de convertir los datos categoricos en general para aplicar estos datos en modelos de machine learning, ademas de verificar el balanceo de datos y aplicacion de tecnicas para corregir variables con outlayers

```{python}
for column in df.columns:
    unique_values = df[column].unique()
    print(f"Valores únicos en la columna '{column}':")
    print(unique_values)
    print("\n------------------------------------\n")
```

## LabelEncoder
### Parental_Involvement, Access_to_Resources, Motivation_Level,Family_Income,Teacher_Quality,Peer_Influence,
```{python}
le = LabelEncoder()
df['Parental_Involvement'] = le.fit_transform(df['Parental_Involvement'].astype(str))

df['Access_to_Resources'] = le.fit_transform(df['Access_to_Resources'].astype(str))

df['Motivation_Level'] = le.fit_transform(df['Motivation_Level'].astype(str))

df['Family_Income'] = le.fit_transform(df['Family_Income'].astype(str))

df['Teacher_Quality'] = le.fit_transform(df['Teacher_Quality'].astype(str))

df['Peer_Influence'] = le.fit_transform(df['Peer_Influence'].astype(str))

df['Parental_Education_Level'] = le.fit_transform(df['Parental_Education_Level'].astype(str))

df['Distance_from_Home'] = le.fit_transform(df['Distance_from_Home'].astype(str))

df

```

## OneHotEncoder

### Internet_Access, Extracurricular_Activities, School_Type, Gender, ,Peer_Influence
```{python}
# Aplicar One-Hot Encoding
df = pd.get_dummies(df, columns=['Extracurricular_Activities'])
df = pd.get_dummies(df, columns=['Internet_Access'])
df = pd.get_dummies(df, columns=['School_Type'])
df = pd.get_dummies(df, columns=['Peer_Influence'])
df = pd.get_dummies(df, columns=['Learning_Disabilities'])
df = pd.get_dummies(df, columns=['Gender'])

```

```{python}
for column in df.columns:
    unique_values = df[column].unique()
    print(f"Valores únicos en la columna '{column}':")
    print(unique_values)
    print("\n------------------------------------\n")
```


```{python}
# Calcular la matriz de correlación
corr_matrix = df.corr()

# Generar el mapa de calor
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Mapa de calor de la correlación entre variables')
plt.show()
```

```{python}
# Calcular la matriz de correlación
correlation_matrix = df.corr()

# Filtrar solo las correlaciones con la variable objetivo
target_corr = correlation_matrix['Exam_Score'].sort_values(ascending=False)
print("Correlación de cada variable con 'Exam_Score':\n", target_corr)
```

# Aplicacion de modelos de machine learning

```{python}
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Separar las características (X) y la variable objetivo (y)
X = df.drop(columns=['Exam_Score'])  # Todas las columnas excepto la variable objetivo
y = df['Exam_Score']  # La variable objetivo

# Dividir el conjunto de datos en conjunto de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Lista de modelos de regresión para evaluar
models = {
    "Regresión Lineal": LinearRegression(),
    "Regresión Ridge": Ridge(alpha=1.0),
    "Regresión Lasso": Lasso(alpha=0.1),
    "Árbol de Decisión": DecisionTreeRegressor(random_state=42,),
    "Random Forest": RandomForestRegressor(n_estimators=100, random_state=42)
}
results = []

# Entrenar y evaluar cada modelo
for model_name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    
    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)
    
    # Almacenar los resultados
    results.append({
        "Modelo": model_name,
        "MAE": mae,
        "RMSE": rmse,
        "R^2": r2
    })

# Convertir los resultados en un DataFrame
results_df = pd.DataFrame(results)

# Mostrar los resultados ordenados por R^2
print(results_df.sort_values(by="R^2", ascending=False))

```
```{python}
# Variables utilizadas
print("Variables utilizadas en el modelo:")
print(X.columns)

```

```{python}
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, StackingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Separar las variables independientes y la variable objetivo
X = df.drop(columns=['Exam_Score','Parental_Involvement','Access_to_Resources','Peer_Influence_0','Learning_Disabilities_Yes','Distance_from_Home',
'Learning_Disabilities_No'
,'Peer_Influence_2'
,'Extracurricular_Activities_Yes'
,'Internet_Access_Yes'
,'Parental_Education_Level'
,'Physical_Activity'
,'School_Type_Private'
,'Gender_Female'
,'Gender_Male'
,'Peer_Influence_1'
,'School_Type_Public'
,'Motivation_Level'
,'Sleep_Hours'
,'Family_Income'
,'Internet_Access_No'
,'Teacher_Quality'
,'Extracurricular_Activities_No'])
y = df['Exam_Score']

# Dividir el conjunto de datos
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Definir los modelos base
base_models = [
    ('linear', LinearRegression()),
    ('random_forest', RandomForestRegressor(n_estimators=100, random_state=42))
]

# Definir el modelo meta
meta_model = LinearRegression()

# Crear el Stacking Regressor
stacking_model = StackingRegressor(estimators=base_models, final_estimator=meta_model)

# Entrenar el modelo
stacking_model.fit(X_train, y_train)

# Hacer predicciones
y_pred = stacking_model.predict(X_test)

# Evaluar el modelo
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

# Mostrar resultados
print("Resultados del Stacking Regressor")
print(f"MAE: {mae:.4f}")
print(f"RMSE: {rmse:.4f}")
print(f"R^2: {r2:.4f}")


```